<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-03-05T20:50:09+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Joon</title><subtitle>## 아름답고 쓸모 없기를
</subtitle><author><name>Jungjoon Park</name><email>biasdrive@gmail.com</email></author><entry><title type="html">Ensemble</title><link href="http://localhost:4000/blog/2025-03-05-Ensemble-copy/" rel="alternate" type="text/html" title="Ensemble" /><published>2025-03-05T00:00:00+09:00</published><updated>2025-03-05T20:34:40+09:00</updated><id>http://localhost:4000/blog/Ensemble%20copy</id><content type="html" xml:base="http://localhost:4000/blog/2025-03-05-Ensemble-copy/"><![CDATA[<ul>
  <li><strong>개념</strong>
    <ul>
      <li><strong>여러 개의 모델(약한 학습자)을 결합</strong>하여 하나의 강력한 예측 모델을 만드는 방법.</li>
    </ul>
  </li>
  <li>종류
    <ul>
      <li><strong>배깅(Bagging: Bootstrap Aggregating)</strong>
        <ul>
          <li>개념
            <ul>
              <li>
                <p>원본 데이터에서 <strong>복원 추출한 여러 샘플</strong>로 <strong>여러 모델</strong>을 학습하고,</p>
              </li>
              <li>
                <p>그 <strong>예측을 평균(회귀)하거나 투표(분류)하여 최종</strong> 예측을 수행</p>
              </li>
            </ul>
          </li>
          <li>단계
            <ul>
              <li><strong>데이터 샘플링:</strong> 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성.</li>
              <li><strong>모델 학습:</strong> 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출.</li>
            </ul>
          </li>
          <li>대표 알고리즘
            <ul>
              <li><strong>랜덤 포레스트(Random Forest)</strong>
                <ul>
                  <li>배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>부스팅(Boosting)</strong>
        <ul>
          <li>개념
            <ul>
              <li>약한 학습자를 <strong>순차적</strong>으로 학습하며, 이전 모델이 <strong>잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여</strong>나가는 방법.</li>
            </ul>
          </li>
          <li><strong>단계</strong>
            <ul>
              <li><strong>순차적 학습:</strong> 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완.</li>
              <li><strong>가중치 조정:</strong> 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출.
                <h2 id="1-앙상블-학습-개요">1. 앙상블 학습 개요</h2>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>앙상블 학습은 여러 모델의 예측 결과를 결합하여 일반화 성능을 높이는 기법으로, 주로 Bagging과 Boosting 두 가지 방식이 있다.</p>

<h3 id="주요-개념">주요 개념</h3>

<ul>
  <li><strong>Bootstrap</strong>:
    <ul>
      <li>복원 추출을 사용하여 표본을 생성하고 모집단 통계량을 추론하는 통계적 방법.</li>
      <li>복원 추출을 통해 각 표본이 독립적으로 구성됨.</li>
    </ul>
  </li>
  <li><strong>Tree Ensemble</strong>:
    <ul>
      <li>Bagging과 Boosting을 기반으로 한 결정 트리 기반 앙상블 기법.</li>
    </ul>
  </li>
  <li><strong>결과 결합 방법</strong>:
    <ul>
      <li><strong>Voting</strong>:
        <ul>
          <li>분류 문제(categorical)에서 다수결로 최종 결과 결정.</li>
        </ul>
      </li>
      <li><strong>Average</strong>:
        <ul>
          <li>회귀 문제(continuous)에서 평균값으로 최종 결과 계산.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="2-bagging-bootstrap--aggregation">2. Bagging (Bootstrap + Aggregation)</h2>

<ul>
  <li><strong>정의</strong>:복원 추출로 생성된 여러 데이터 서브셋을 독립적으로 학습시키고 결과를 결합하는 방식.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>Bootstrap 샘플링으로 데이터 서브셋 생성.</li>
      <li>각 서브셋을 독립적인 모델로 학습.</li>
      <li>결과를 Aggregation(voting/average)하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 모델</strong>:<strong>랜덤 포레스트(Random Forest)</strong>.</li>
  <li><strong>장점</strong>:
    <ul>
      <li>분산 감소로 일반화 성능 향상.</li>
      <li>과적합 완화.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="3-boosting">3. Boosting</h2>

<ul>
  <li><strong>정의</strong>:이전 모델의 예측 오류를 개선하는 방식으로 모델을 순차적으로 학습하며 성능을 점진적으로 향상.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>첫 모델 학습 후 예측 오류(Residual)를 계산.</li>
      <li>오류를 개선하는 방향으로 다음 모델 학습.</li>
      <li>가중치를 조정하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 알고리즘</strong>:
    <ul>
      <li><strong>에이다부스트(AdaBoost)</strong>:이전 모델의 오류에 가중치를 부여해 다음 모델 학습에 반영.</li>
      <li><strong>그라디언트 부스팅 머신(GBM)</strong>:손실 함수의 그래디언트에 따라 잔차를 줄이는 방향으로 학습.
        <ul>
          <li><strong>XGBoost</strong>: GBM의 개선형으로 학습 속도와 정확도 향상.</li>
          <li><strong>LightGBM</strong>: 대규모 데이터셋에 최적화.</li>
          <li><strong>CatBoost</strong>: 범주형 데이터 처리에 특화.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>장점</strong>:
    <ul>
      <li>높은 예측 정확도.</li>
      <li>점진적 성능 향상.</li>
    </ul>
  </li>
  <li><strong>단점</strong>:
    <ul>
      <li>계산 비용이 높음.</li>
      <li>과적합 가능성.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="4-bagging-vs-boosting-비교">4. Bagging vs. Boosting 비교</h2>

<table>
  <thead>
    <tr>
      <th>특징</th>
      <th>Bagging</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>데이터 구성</td>
      <td>독립적 표본 구성 (복원 추출)</td>
      <td>이전 모델 오류를 기반으로 구성</td>
    </tr>
    <tr>
      <td>모델 학습</td>
      <td>각 모델 독립적으로 학습</td>
      <td>이전 모델의 성능 영향을 받음</td>
    </tr>
    <tr>
      <td>목적</td>
      <td>분산 감소</td>
      <td>편향 감소</td>
    </tr>
    <tr>
      <td>결과 결합 방식</td>
      <td>Voting</td>
      <td>Weighted Averaging</td>
    </tr>
  </tbody>
</table>]]></content><author><name>Jungjoon Park</name><email>biasdrive@gmail.com</email></author><category term="blog" /><summary type="html"><![CDATA[개념 여러 개의 모델(약한 학습자)을 결합하여 하나의 강력한 예측 모델을 만드는 방법. 종류 배깅(Bagging: Bootstrap Aggregating) 개념 원본 데이터에서 복원 추출한 여러 샘플로 여러 모델을 학습하고, 그 예측을 평균(회귀)하거나 투표(분류)하여 최종 예측을 수행 단계 데이터 샘플링: 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성. 모델 학습: 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습. 예측 결합: 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출. 대표 알고리즘 랜덤 포레스트(Random Forest) 배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용. 부스팅(Boosting) 개념 약한 학습자를 순차적으로 학습하며, 이전 모델이 잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여나가는 방법. 단계 순차적 학습: 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완. 가중치 조정: 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습. 예측 결합: 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출. 1. 앙상블 학습 개요]]></summary></entry><entry><title type="html">Ensemble</title><link href="http://localhost:4000/blog/2025-03-05-Ensemble/" rel="alternate" type="text/html" title="Ensemble" /><published>2025-03-05T00:00:00+09:00</published><updated>2025-03-05T19:53:36+09:00</updated><id>http://localhost:4000/blog/Ensemble</id><content type="html" xml:base="http://localhost:4000/blog/2025-03-05-Ensemble/"><![CDATA[<ul>
  <li><strong>개념</strong>
    <ul>
      <li><strong>여러 개의 모델(약한 학습자)을 결합</strong>하여 하나의 강력한 예측 모델을 만드는 방법.</li>
    </ul>
  </li>
  <li>종류
    <ul>
      <li><strong>배깅(Bagging: Bootstrap Aggregating)</strong>
        <ul>
          <li>개념
            <ul>
              <li>
                <p>원본 데이터에서 <strong>복원 추출한 여러 샘플</strong>로 <strong>여러 모델</strong>을 학습하고,</p>
              </li>
              <li>
                <p>그 <strong>예측을 평균(회귀)하거나 투표(분류)하여 최종</strong> 예측을 수행</p>
              </li>
            </ul>
          </li>
          <li>단계
            <ul>
              <li><strong>데이터 샘플링:</strong> 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성.</li>
              <li><strong>모델 학습:</strong> 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출.</li>
            </ul>
          </li>
          <li>대표 알고리즘
            <ul>
              <li><strong>랜덤 포레스트(Random Forest)</strong>
                <ul>
                  <li>배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>부스팅(Boosting)</strong>
        <ul>
          <li>개념
            <ul>
              <li>약한 학습자를 <strong>순차적</strong>으로 학습하며, 이전 모델이 <strong>잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여</strong>나가는 방법.</li>
            </ul>
          </li>
          <li><strong>단계</strong>
            <ul>
              <li><strong>순차적 학습:</strong> 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완.</li>
              <li><strong>가중치 조정:</strong> 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출.
                <h2 id="1-앙상블-학습-개요">1. 앙상블 학습 개요</h2>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>앙상블 학습은 여러 모델의 예측 결과를 결합하여 일반화 성능을 높이는 기법으로, 주로 Bagging과 Boosting 두 가지 방식이 있다.</p>

<h3 id="주요-개념">주요 개념</h3>

<ul>
  <li><strong>Bootstrap</strong>:
    <ul>
      <li>복원 추출을 사용하여 표본을 생성하고 모집단 통계량을 추론하는 통계적 방법.</li>
      <li>복원 추출을 통해 각 표본이 독립적으로 구성됨.</li>
    </ul>
  </li>
  <li><strong>Tree Ensemble</strong>:
    <ul>
      <li>Bagging과 Boosting을 기반으로 한 결정 트리 기반 앙상블 기법.</li>
    </ul>
  </li>
  <li><strong>결과 결합 방법</strong>:
    <ul>
      <li><strong>Voting</strong>:
        <ul>
          <li>분류 문제(categorical)에서 다수결로 최종 결과 결정.</li>
        </ul>
      </li>
      <li><strong>Average</strong>:
        <ul>
          <li>회귀 문제(continuous)에서 평균값으로 최종 결과 계산.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="2-bagging-bootstrap--aggregation">2. Bagging (Bootstrap + Aggregation)</h2>

<ul>
  <li><strong>정의</strong>:복원 추출로 생성된 여러 데이터 서브셋을 독립적으로 학습시키고 결과를 결합하는 방식.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>Bootstrap 샘플링으로 데이터 서브셋 생성.</li>
      <li>각 서브셋을 독립적인 모델로 학습.</li>
      <li>결과를 Aggregation(voting/average)하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 모델</strong>:<strong>랜덤 포레스트(Random Forest)</strong>.</li>
  <li><strong>장점</strong>:
    <ul>
      <li>분산 감소로 일반화 성능 향상.</li>
      <li>과적합 완화.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="3-boosting">3. Boosting</h2>

<ul>
  <li><strong>정의</strong>:이전 모델의 예측 오류를 개선하는 방식으로 모델을 순차적으로 학습하며 성능을 점진적으로 향상.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>첫 모델 학습 후 예측 오류(Residual)를 계산.</li>
      <li>오류를 개선하는 방향으로 다음 모델 학습.</li>
      <li>가중치를 조정하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 알고리즘</strong>:
    <ul>
      <li><strong>에이다부스트(AdaBoost)</strong>:이전 모델의 오류에 가중치를 부여해 다음 모델 학습에 반영.</li>
      <li><strong>그라디언트 부스팅 머신(GBM)</strong>:손실 함수의 그래디언트에 따라 잔차를 줄이는 방향으로 학습.
        <ul>
          <li><strong>XGBoost</strong>: GBM의 개선형으로 학습 속도와 정확도 향상.</li>
          <li><strong>LightGBM</strong>: 대규모 데이터셋에 최적화.</li>
          <li><strong>CatBoost</strong>: 범주형 데이터 처리에 특화.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>장점</strong>:
    <ul>
      <li>높은 예측 정확도.</li>
      <li>점진적 성능 향상.</li>
    </ul>
  </li>
  <li><strong>단점</strong>:
    <ul>
      <li>계산 비용이 높음.</li>
      <li>과적합 가능성.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="4-bagging-vs-boosting-비교">4. Bagging vs. Boosting 비교</h2>

<table>
  <thead>
    <tr>
      <th>특징</th>
      <th>Bagging</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>데이터 구성</td>
      <td>독립적 표본 구성 (복원 추출)</td>
      <td>이전 모델 오류를 기반으로 구성</td>
    </tr>
    <tr>
      <td>모델 학습</td>
      <td>각 모델 독립적으로 학습</td>
      <td>이전 모델의 성능 영향을 받음</td>
    </tr>
    <tr>
      <td>목적</td>
      <td>분산 감소</td>
      <td>편향 감소</td>
    </tr>
    <tr>
      <td>결과 결합 방식</td>
      <td>Voting</td>
      <td>Weighted Averaging</td>
    </tr>
  </tbody>
</table>]]></content><author><name>Jungjoon Park</name><email>biasdrive@gmail.com</email></author><category term="blog" /><summary type="html"><![CDATA[개념 여러 개의 모델(약한 학습자)을 결합하여 하나의 강력한 예측 모델을 만드는 방법. 종류 배깅(Bagging: Bootstrap Aggregating) 개념 원본 데이터에서 복원 추출한 여러 샘플로 여러 모델을 학습하고, 그 예측을 평균(회귀)하거나 투표(분류)하여 최종 예측을 수행 단계 데이터 샘플링: 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성. 모델 학습: 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습. 예측 결합: 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출. 대표 알고리즘 랜덤 포레스트(Random Forest) 배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용. 부스팅(Boosting) 개념 약한 학습자를 순차적으로 학습하며, 이전 모델이 잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여나가는 방법. 단계 순차적 학습: 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완. 가중치 조정: 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습. 예측 결합: 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출. 1. 앙상블 학습 개요]]></summary></entry><entry><title type="html">X Marks the Spot in Hydejack 9.2</title><link href="http://localhost:4000/blog/2024-09-08-x-marks-the-spot-in-hydejack-9-2/" rel="alternate" type="text/html" title="X Marks the Spot in Hydejack 9.2" /><published>2024-09-08T00:00:00+09:00</published><updated>2025-03-05T19:53:36+09:00</updated><id>http://localhost:4000/blog/x-marks-the-spot-in-hydejack-9-2</id><content type="html" xml:base="http://localhost:4000/blog/2024-09-08-x-marks-the-spot-in-hydejack-9-2/"><![CDATA[<h2 id="new-social-media-icons">New Social Media Icons</h2>

<p>Lots of things have changed in the world of social media since the last release. To bring Hydejack up to date, the default logo for Twitter has changed:</p>

<p class="larger"><span class="icon-twitter-old"></span> → <span class="icon-twitter"></span></p>

<!--more-->

<ul class="large-only" id="markdown-toc">
  <li><a href="#new-social-media-icons" id="markdown-toc-new-social-media-icons">New Social Media Icons</a></li>
  <li><a href="#dark-mode-is-now-free" id="markdown-toc-dark-mode-is-now-free">Dark Mode is Now Free</a></li>
  <li><a href="#updated-docs" id="markdown-toc-updated-docs">Updated Docs</a></li>
  <li><a href="#google-fonts-off-by-default" id="markdown-toc-google-fonts-off-by-default">Google Fonts Off by Default</a></li>
</ul>

<p class="note smaller">If you prefer the old Twitter logo, you can use it through <code class="language-plaintext highlighter-rouge">twitter-old</code>.</p>

<p>There are also many new social media networks, some of which are now included by default:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Name</th>
      <th>Icon</th>
      <th style="text-align: left">Name</th>
      <th>Icon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">signal</td>
      <td><span class="larger icon-signal"></span></td>
      <td style="text-align: left">threads</td>
      <td><span class="larger icon-threads"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">playstation</td>
      <td><span class="larger icon-playstation"></span></td>
      <td style="text-align: left">messenger</td>
      <td><span class="larger icon-messenger"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">stripe</td>
      <td><span class="larger icon-stripe"></span></td>
      <td style="text-align: left">slack</td>
      <td><span class="larger icon-slack"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">gitlab</td>
      <td><span class="larger icon-gitlab"></span></td>
      <td style="text-align: left">line</td>
      <td><span class="larger icon-line"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">medium</td>
      <td><span class="larger icon-medium"></span></td>
      <td style="text-align: left">xbox</td>
      <td><span class="larger icon-xbox"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">wechat</td>
      <td><span class="larger icon-wechat"></span></td>
      <td style="text-align: left">discord</td>
      <td><span class="larger icon-discord"></span></td>
    </tr>
    <tr>
      <td style="text-align: left">mastodon</td>
      <td><span class="larger icon-mastodon"></span></td>
      <td style="text-align: left">twitter</td>
      <td><span class="larger icon-twitter"></span></td>
    </tr>
  </tbody>
</table>

<p>If your perferred network is missing, note that you can always <a href="../../docs/advanced.md#adding-a-custom-social-media-icon">follow the steps to add custom icons</a> from the docs, which is what I did for this release.</p>

<h2 id="dark-mode-is-now-free">Dark Mode is Now Free</h2>
<p>When I first added dark mode to Hydejack it was still considered a novelty. 
Unity, a popular game engine, was limiting dark mode to its paid version at the time — a model that I’ve adopted for Hydejack. 
Today, dark mode is considered a minimal requirement for any new theme and to reflect that reality, 
starting with Hydejack 9.2, dark mode is included in all versions of Hydejack.</p>

<h2 id="updated-docs">Updated Docs</h2>
<p>The documentation has been updated with a focus on deployment via GitHub Actions and CI pipelines. 
I’ve added a chapter on how to <a href="../../docs/deploy.md" class="heading flip-title">Deploy</a> and updated many of the existing chapters.</p>

<p>The deployment experience for <strong>PRO customers</strong> has also been improved. You are now automatically added to a “PRO Customers” team on GitHub if you provide a GitHub username during checkout (existing customers can request an invite through <a href="mailto:mail@hydejack.com">mail@hydejack.com</a>).
Members of this team have read access to the pro repository, which allows the theme to be fetched during a CI run. 
For detail, check out the new <a href="../../docs/deploy.md" class="heading flip-title">Deploy</a> chapter.</p>

<h2 id="google-fonts-off-by-default">Google Fonts Off by Default</h2>
<p>Google Fonts are now turned off by default in the starter kits, but remain in use on hydejack.com for visual continuity. All associated options continue to work as they did before. Only new users have to enable them in the config file if they want to match the look of hydejack.com.</p>

<p>The reason for this change is that sensibilities around privacy have changed in recent years. 
No Google product feels appropriate as a default option for an ownership and self-hosting oriented product like Hydejack.</p>

<p>To restore the old look that matches hydejack.com, add the following to your <code class="language-plaintext highlighter-rouge">_config.yml</code> file:</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">google_fonts</span><span class="pi">:</span>          <span class="s">Roboto+Slab:700|Noto+Sans:400,400i,700,700i</span>
<span class="na">font</span><span class="pi">:</span>                  <span class="s">Noto Sans, Helvetica, Arial, sans-serif</span>
<span class="na">font_heading</span><span class="pi">:</span>          <span class="s">Roboto Slab, Helvetica, Arial, sans-serif</span>
</code></pre></div></div>

<p>On a related note, I’ve also decided against updating the included Google Analytics script, in part because the upgrade path is incomprehensible, but also due the the same privacy concerns that make Google Fonts a bad default option. I recommend independent analytics services like 
<a href="https://plausible.io">Plausible</a>, <a href="https://matomo.org/">Matomo</a> or maybe even <a href="https://counterscale.dev">Counterscale</a> (if you are a Cloudflare customer).
You can include the tracking scripts by <a href="../../docs/basics.md#adding-custom-html-to-the-head">adding them as custom HTML</a>.</p>]]></content><author><name>Jungjoon Park</name><email>biasdrive@gmail.com</email></author><category term="blog" /><summary type="html"><![CDATA[This feature release adds new social media icons for 2024 and makes dark mode available to everyone.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/pawel-czerwinski-848z7lbCjoo-unsplash.jpg" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/pawel-czerwinski-848z7lbCjoo-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ensemble</title><link href="http://localhost:4000/blog/2022-01-05-ML_Ensemble/" rel="alternate" type="text/html" title="Ensemble" /><published>2022-01-05T00:00:00+09:00</published><updated>2025-03-05T20:34:40+09:00</updated><id>http://localhost:4000/blog/ML_Ensemble</id><content type="html" xml:base="http://localhost:4000/blog/2022-01-05-ML_Ensemble/"><![CDATA[<ul>
  <li><strong>개념</strong>
    <ul>
      <li><strong>여러 개의 모델(약한 학습자)을 결합</strong>하여 하나의 강력한 예측 모델을 만드는 방법.</li>
    </ul>
  </li>
  <li>종류
    <ul>
      <li><strong>배깅(Bagging: Bootstrap Aggregating)</strong>
        <ul>
          <li>개념
            <ul>
              <li>
                <p>원본 데이터에서 <strong>복원 추출한 여러 샘플</strong>로 <strong>여러 모델</strong>을 학습하고,</p>
              </li>
              <li>
                <p>그 <strong>예측을 평균(회귀)하거나 투표(분류)하여 최종</strong> 예측을 수행</p>
              </li>
            </ul>
          </li>
          <li>단계
            <ul>
              <li><strong>데이터 샘플링:</strong> 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성.</li>
              <li><strong>모델 학습:</strong> 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출.</li>
            </ul>
          </li>
          <li>대표 알고리즘
            <ul>
              <li><strong>랜덤 포레스트(Random Forest)</strong>
                <ul>
                  <li>배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>부스팅(Boosting)</strong>
        <ul>
          <li>개념
            <ul>
              <li>약한 학습자를 <strong>순차적</strong>으로 학습하며, 이전 모델이 <strong>잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여</strong>나가는 방법.</li>
            </ul>
          </li>
          <li><strong>단계</strong>
            <ul>
              <li><strong>순차적 학습:</strong> 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완.</li>
              <li><strong>가중치 조정:</strong> 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습.</li>
              <li><strong>예측 결합:</strong> 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출.
                <h2 id="1-앙상블-학습-개요">1. 앙상블 학습 개요</h2>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>앙상블 학습은 여러 모델의 예측 결과를 결합하여 일반화 성능을 높이는 기법으로, 주로 Bagging과 Boosting 두 가지 방식이 있다.</p>

<h3 id="주요-개념">주요 개념</h3>

<ul>
  <li><strong>Bootstrap</strong>:
    <ul>
      <li>복원 추출을 사용하여 표본을 생성하고 모집단 통계량을 추론하는 통계적 방법.</li>
      <li>복원 추출을 통해 각 표본이 독립적으로 구성됨.</li>
    </ul>
  </li>
  <li><strong>Tree Ensemble</strong>:
    <ul>
      <li>Bagging과 Boosting을 기반으로 한 결정 트리 기반 앙상블 기법.</li>
    </ul>
  </li>
  <li><strong>결과 결합 방법</strong>:
    <ul>
      <li><strong>Voting</strong>:
        <ul>
          <li>분류 문제(categorical)에서 다수결로 최종 결과 결정.</li>
        </ul>
      </li>
      <li><strong>Average</strong>:
        <ul>
          <li>회귀 문제(continuous)에서 평균값으로 최종 결과 계산.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="2-bagging-bootstrap--aggregation">2. Bagging (Bootstrap + Aggregation)</h2>

<ul>
  <li><strong>정의</strong>:복원 추출로 생성된 여러 데이터 서브셋을 독립적으로 학습시키고 결과를 결합하는 방식.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>Bootstrap 샘플링으로 데이터 서브셋 생성.</li>
      <li>각 서브셋을 독립적인 모델로 학습.</li>
      <li>결과를 Aggregation(voting/average)하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 모델</strong>:<strong>랜덤 포레스트(Random Forest)</strong>.</li>
  <li><strong>장점</strong>:
    <ul>
      <li>분산 감소로 일반화 성능 향상.</li>
      <li>과적합 완화.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="3-boosting">3. Boosting</h2>

<ul>
  <li><strong>정의</strong>:이전 모델의 예측 오류를 개선하는 방식으로 모델을 순차적으로 학습하며 성능을 점진적으로 향상.</li>
  <li><strong>작동 원리</strong>:
    <ol>
      <li>첫 모델 학습 후 예측 오류(Residual)를 계산.</li>
      <li>오류를 개선하는 방향으로 다음 모델 학습.</li>
      <li>가중치를 조정하여 최종 예측 생성.</li>
    </ol>
  </li>
  <li><strong>대표 알고리즘</strong>:
    <ul>
      <li><strong>에이다부스트(AdaBoost)</strong>:이전 모델의 오류에 가중치를 부여해 다음 모델 학습에 반영.</li>
      <li><strong>그라디언트 부스팅 머신(GBM)</strong>:손실 함수의 그래디언트에 따라 잔차를 줄이는 방향으로 학습.
        <ul>
          <li><strong>XGBoost</strong>: GBM의 개선형으로 학습 속도와 정확도 향상.</li>
          <li><strong>LightGBM</strong>: 대규모 데이터셋에 최적화.</li>
          <li><strong>CatBoost</strong>: 범주형 데이터 처리에 특화.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>장점</strong>:
    <ul>
      <li>높은 예측 정확도.</li>
      <li>점진적 성능 향상.</li>
    </ul>
  </li>
  <li><strong>단점</strong>:
    <ul>
      <li>계산 비용이 높음.</li>
      <li>과적합 가능성.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="4-bagging-vs-boosting-비교">4. Bagging vs. Boosting 비교</h2>

<table>
  <thead>
    <tr>
      <th>특징</th>
      <th>Bagging</th>
      <th>Boosting</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>데이터 구성</td>
      <td>독립적 표본 구성 (복원 추출)</td>
      <td>이전 모델 오류를 기반으로 구성</td>
    </tr>
    <tr>
      <td>모델 학습</td>
      <td>각 모델 독립적으로 학습</td>
      <td>이전 모델의 성능 영향을 받음</td>
    </tr>
    <tr>
      <td>목적</td>
      <td>분산 감소</td>
      <td>편향 감소</td>
    </tr>
    <tr>
      <td>결과 결합 방식</td>
      <td>Voting</td>
      <td>Weighted Averaging</td>
    </tr>
  </tbody>
</table>]]></content><author><name>Jungjoon Park</name><email>biasdrive@gmail.com</email></author><category term="blog" /><summary type="html"><![CDATA[개념 여러 개의 모델(약한 학습자)을 결합하여 하나의 강력한 예측 모델을 만드는 방법. 종류 배깅(Bagging: Bootstrap Aggregating) 개념 원본 데이터에서 복원 추출한 여러 샘플로 여러 모델을 학습하고, 그 예측을 평균(회귀)하거나 투표(분류)하여 최종 예측을 수행 단계 데이터 샘플링: 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성. 모델 학습: 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습. 예측 결합: 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출. 대표 알고리즘 랜덤 포레스트(Random Forest) 배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용. 부스팅(Boosting) 개념 약한 학습자를 순차적으로 학습하며, 이전 모델이 잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여나가는 방법. 단계 순차적 학습: 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완. 가중치 조정: 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습. 예측 결합: 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출. 1. 앙상블 학습 개요]]></summary></entry></feed>