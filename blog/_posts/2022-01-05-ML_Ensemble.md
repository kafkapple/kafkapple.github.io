# Ensemble

- **개념**
    - **여러 개의 모델(약한 학습자)을 결합**하여 하나의 강력한 예측 모델을 만드는 방법.
- 종류
    - **배깅(Bagging: Bootstrap Aggregating)**
        - 개념
            - 원본 데이터에서 **복원 추출한 여러 샘플**로 **여러 모델**을 학습하고,
            
            - 그 **예측을 평균(회귀)하거나 투표(분류)하여 최종** 예측을 수행
        - 단계
            - **데이터 샘플링:** 원본 데이터에서 복원 추출로 여러 개의 부트스트랩 샘플을 생성.
            - **모델 학습:** 각 샘플로 개별 모델(약한 학습자)을 독립적으로 학습.
            - **예측 결합:** 개별 모델의 예측을 평균 또는 다수결로 결합하여 최종 예측을 산출.
        - 대표 알고리즘
            - **랜덤 포레스트(Random Forest)**
                - 배깅과 무작위 특징 선택을 결합한 알고리즘으로, 여러 개의 결정 트리를 사용.
    - **부스팅(Boosting)**
        - 개념
            - 약한 학습자를 **순차적**으로 학습하며, 이전 모델이 **잘못 예측한 샘플에 가중치를 부여하여 오차를 줄여**나가는 방법.
        - **단계**
            - **순차적 학습:** 모델을 하나씩 순서대로 학습하며, 이전 모델의 오류를 다음 모델이 보완.
            - **가중치 조정:** 잘못 예측한 샘플에 더 큰 가중치를 부여하여 다음 모델이 집중 학습.
            - **예측 결합:** 개별 모델의 예측을 가중치 합산하여 최종 예측을 산출.
  ## 1. 앙상블 학습 개요

앙상블 학습은 여러 모델의 예측 결과를 결합하여 일반화 성능을 높이는 기법으로, 주로 Bagging과 Boosting 두 가지 방식이 있다.

### 주요 개념

- **Bootstrap**:
    - 복원 추출을 사용하여 표본을 생성하고 모집단 통계량을 추론하는 통계적 방법.
    - 복원 추출을 통해 각 표본이 독립적으로 구성됨.
- **Tree Ensemble**:
    - Bagging과 Boosting을 기반으로 한 결정 트리 기반 앙상블 기법.
- **결과 결합 방법**:
    - **Voting**:
        - 분류 문제(categorical)에서 다수결로 최종 결과 결정.
    - **Average**:
        - 회귀 문제(continuous)에서 평균값으로 최종 결과 계산.

---

## 2. Bagging (Bootstrap + Aggregation)

- **정의**:복원 추출로 생성된 여러 데이터 서브셋을 독립적으로 학습시키고 결과를 결합하는 방식.
- **작동 원리**:
    1. Bootstrap 샘플링으로 데이터 서브셋 생성.
    2. 각 서브셋을 독립적인 모델로 학습.
    3. 결과를 Aggregation(voting/average)하여 최종 예측 생성.
- **대표 모델**:**랜덤 포레스트(Random Forest)**.
- **장점**:
    - 분산 감소로 일반화 성능 향상.
    - 과적합 완화.

---

## 3. Boosting

- **정의**:이전 모델의 예측 오류를 개선하는 방식으로 모델을 순차적으로 학습하며 성능을 점진적으로 향상.
- **작동 원리**:
    1. 첫 모델 학습 후 예측 오류(Residual)를 계산.
    2. 오류를 개선하는 방향으로 다음 모델 학습.
    3. 가중치를 조정하여 최종 예측 생성.
- **대표 알고리즘**:
    - **에이다부스트(AdaBoost)**:이전 모델의 오류에 가중치를 부여해 다음 모델 학습에 반영.
    - **그라디언트 부스팅 머신(GBM)**:손실 함수의 그래디언트에 따라 잔차를 줄이는 방향으로 학습.
        - **XGBoost**: GBM의 개선형으로 학습 속도와 정확도 향상.
        - **LightGBM**: 대규모 데이터셋에 최적화.
        - **CatBoost**: 범주형 데이터 처리에 특화.
- **장점**:
    - 높은 예측 정확도.
    - 점진적 성능 향상.
- **단점**:
    - 계산 비용이 높음.
    - 과적합 가능성.

---

## 4. Bagging vs. Boosting 비교

| 특징 | Bagging | Boosting |
| --- | --- | --- |
| 데이터 구성 | 독립적 표본 구성 (복원 추출) | 이전 모델 오류를 기반으로 구성 |
| 모델 학습 | 각 모델 독립적으로 학습 | 이전 모델의 성능 영향을 받음 |
| 목적 | 분산 감소 | 편향 감소 |
| 결과 결합 방식 | Voting | Weighted Averaging |
